---
title: "Module04Sandbox"
author: "Diana McSpadden"
date: "9/20/2020"
output: html_document
---

# Module 4: Introduction to Multiple Linear Regression

MLR: Multiple linear regression == multiple predictor variables on a single response variable


## Essential Questions

1. How is the multiple linear regression model set up?

2. How do we use a multiple linear regression model to assess the relationship between the response variable and the multiple predictors?

3. How similar and different are the MLR and SLR models?

## Learning 

1.  the setup of the multiple linear regression model.

2. Compare and contrast the multiple linear regression model with the simple linear regression model, especially in terms of **interpreting model coefficients** and **results from hypothesis tests** and **confidence intervals**.

3. Use a multiple linear regression model to answer questions regarding the relationship between a quantitative response variable and several predictors, and make predictions.

4. Describe what questions are addressed by the various confidence intervals and hypothesis tests in a multiple linear regression. 

5. Given a data set and a question of interest, **identify what inferential procedure to use in a multiple linear regression setting.**

6. Assess the appropriateness of your multiple linear regression model for data analysis and apply remedial measures to address common problems in building regression models.

## Reading: 
Sections 3.1 to 3.3.2, 3.4, 3.5.

## 4.1: Introduction to the Lesson

MLR models allow us to examine the effect of multiple predictors on the response variable simultaneously.

Q1: How similar is the MLR model to the SLR model? 

**Assumptions Checked With Plots??**
Residual plot (not scatterplot) - response on y axis, fitted values on x axis. (no predictor variables on residual plot)

Q2: What are the differences between the MLR model and the SLR model?

## 4.2: Multiple Linear Regression

Read Section 3.1

**Write down the multiple linear regression equation and the multiple linear regression model.**

MLR Equation:
?????

E(y) = B0 + B1x1 + B2x2 + ... + Bnxn

MLR Module:

y = B0 + B1x1 + B2x2 + ... + Bkxk + e

**Compare and contrast the multiple linear regression equation and the multiple linear regression model. What do each of these describe?**

the MLR module describe k-dimensional space of the regressor variables: x1 ... xj

empirical models == approximating models (adequate of certain ranges of the regressor variables)

Can include polynomials.

Can include **interaction effects**: y = B0 + B1x1 + B2x2 + B12x1x2 + e

Even contour plots that are not linear are considered MLR.

Polynomials and Interaction effects are okay.

Similarities are that you can describe the relationship in the y = B0 + B1x1 + B2x2 + ... + Bnxn notation.


**How do we interpret the (partial) regression parameter βj in multiple linear regression, and how does this differ from the interpretation in simple linear regression?**

B0 = if all xi = 0 are in the range of the data, then B0 is mean value of y when xi(0..j) are == 0.

Bj = the expected change in y for xj when all other x's are held constant, i.e. xi(i != j)

## 4.3 Estimating Regression Coefficients in Multiple Linear Regression

### Matrix Notation

Two m x n matrices, A and B. (m rows, n columns)

A(m x n) and B(m x n)

entries in matrix A are aij

entires in matrix B are bij

#### Matrix Addition
can only happen when two matrices have the same dimensions.
Add each aij to each bij

A + B = C where cij = aij + bij

Matrix Addition is communtative and associative.

#### Matrix Multiplication
number of columns in first matrix must equal number of rows in second matrix
 A(m x n)   E(n x q)
 
**Read Section 3.2**

**1. Write down the assumptions for a multiple linear regression model.** You may state the assumptions using equations, and then describe what these equations mean. How do these assumptions differ from simple linear regression? Explain how you would assess if the regression assumptions are met.

**Assumptions**
1. E(e) = 0

2. Var(e) = sigma^2

3. x1,x2,...,xk measured without error

4. obeservations on each regressor are independent

5. distribution of regressors does not depend on regression coefficients (B's) or sigma^2

6. distribution of y given x1,x2,...,xk is normal wiht mean B0 + B1x1 + B2x2 + ... + Bkxk

7. Variance of y os sigma^2



**2. Write down the multiple linear regression model using matrix notation. Be sure to write what each matrix is.**

p = k + 1 == the number of normal equations to calculate the regressors (B0 ... Bk) == k + 1

y = XB + e

y = vector, or (n x 1) matrix of response values

X  = (n x p) matrix of a (n x 1) identity matrix and regressor values for each "observation" row.

B = (p x n) matrix of regression coefficients (B0 - Bk)

e = (n x 1) vector of random errors for each observation


**3. Write down the least-squares estimator of β.**

**B-hat = ((X'X)^(-1)) * (X'y)**
  
    provided  (X'X)^-1 exists, it will exist if regressors are linearly independent
    
    (X'X) is a p x p symmetric matrix and X'y is a (p x 1) column vector
    
    diagonal elements of X'X are the sum of squares of the elements in the columns of X
    
    **off-diagonal elements are the sums of the cross products of the elements in columns of X** - NEED HELP UNDERSTANDING SIGNIFICANCE
    
    X'y are the sums of cross products of the columns of X and the observations yi (== each regressor * y observations)

```{r sampleRMatrix}
A <- matrix(c(2, 4, 3, 1, 5, 7,4,5,6,8,5,3), nrow=2, ncol=3, byrow = TRUE)        # fill matrix by rows 
A

A.trans = t(A) # t() is matrix transpose
A.trans

A.A.trans.inv = (A %*% A.trans)^-1 # %*% is matrix multiplication
A.A.trans.inv
```


**4. Write down the hat matrix using matrix notation.**

Bottom of page 73

**hat matrix is the matrix that maps vector of observed values (y column vector) to the vector of fitted values (y-hat column vector)

**H = (X) (X'X))^-1 (X')** 

???? ISN'T THIS (XX') / (X'X) === ???

The difference between observed value and fitted value == residuals == ei = yi = yi-hat

**5. Write down the predicted / fitted values using matrix notation and using the hat matrix.**

y-hat = **Hy**

y-hat = (X)(B-hat)

y-hat = (X) (X'X))^-1 (X')(y)



**6. Write down the residuals using matrix notation and using the hat matrix.**

e = y - X(B-hat)

e = y - Hy

e = (I - H)y

**7. Write down the expected value and variance of the least-squares estimator, β^.**

B-hat = ((X'X)^-1) (X'y)

This results in a (p x 1) column vector That gives B0, B1, ..., Bk

thus, y-hat = (B-hat)(X)

BUT WHAT IS THE **VARIANCE**

**Var(B-hat) = (sigma^2)((X'X)^-1)** page 80


**8. Write down the formula for the estimated variance, σ^2, of a multiple linear regression model.**

As with SLR sigma^2 can be obtained from the SS-E

SS-E = e'e

substituting e = y - X(B-hat)

SS-E = (y'y) - (B-hatX'y)

There are n - p degrees freedom = observations - (residuals + 1) *not surprising, this is how you solve a polynomial)

MS-E = SS-E / (n-p) == sigma^2



How does this differ from simple linear regression?

**Differences
DF is n - p, instead of n - 2?

**In SLR:** sigma^2 = MS-residual == SS-residual / n-2

### QUESTIONS: I get totally lost on page 79.
 
## 4.4: ANOVA F Test in Multiple Linear Regression

**Read Section 3.3.1**

Hypothesis Tests require that: random errors are independent and follow a normal distribution with mean E(e)= 0 and Var(ei) = signma^2

**1. Consider the ANOVA F test in a multiple linear regression setting.**

**1a: In your own words, describe what the ANOVA F test is used for.**

F0 = MS-explained by model / MS-residual errors in model

The fewer unexplained errors, the larger F0 is.

F0 explains the adequacy of our model in comparison to a normal distribution of errors where B1 ... Bj all = 0.

**1b: Write down the null and alternative hypotheses associated with an ANOVA F test.**

H0: B1, B2, ..., Bk = 0

Ha: Bj != 0 for at least one j.

**1c: How do we calculate the ANOVA F statistic? What distribution is the calculated F statistic compared to?**

F0 = MSR / MSE

compare to
F0 > F alpha, k, n-k-1


where k = number of regressors, n = number of observations
```{r}

# run an example with a two-sided alpha 5% k = 5 and n = 8

Fcompare = qf(.95, df1=5, df2=2) 
Fcompare

#FcompareExample = qf(.95, df1 = 5, df2 = 2)
#FcompareExample

```

**2. How is the ANOVA table different in MLR compared to SLR?**

ANOVA table is different with degrees of freedom

Regression df = k, where k is number of regressors

Residual df = n - k - 1, where n is number of observations, k is number of regressors

Total df = n - k - 1 + k = n - 1 degrees freedom


**3. Compare and contrast the R2 and R2adj. What happens when a predictor variable is added? **

R2  = SSR / SST

R2-adj = (SSE / (n-p)) / (SST / n-1))

when a predictor variable is added R2 will never decrease, so it is difficult to interpret R2 increase.

Using R2-adj takes into account new regressors by including n-p, (p = regressor count + 1), and also takes into account the number of observations with including n.


## 4.5: t Test in Multiple Linear Regression

Read Section 3.3.2 (until Example 3.4) of your textbook


**1. When we want to test if a predictor xj can be removed from the model,**

adding a variable to a regression model ALWAYS increases SS-T to increase, and SS-E to decrease. So, the question is, is the increase in SS-R worth the increase in predictive power?

**1a:** How do we write the null and alternative hypotheses? Express the hypotheses using statistical notation, as well as describing in words what those hypotheses mean. 


**1b:** What is the formula for the t statistic?

t0 = Bj-hat / se(Bj-hat)

compare against, t alpha/2, n-k-1 = **qt(.975, n-k-1)**

remember diagonal elements in the (X'X)^-1 is B-hat, so se(Bj-hat) == sqrt((sigma^2 * bjj))

BUT remember this doesn't have us looking in isolation because all other x's have not been held constant.


**1c: **What distribution do we compare the t statistic to? 

compare against, > t alpha/2, n-k-1 = **qt(.975, n-k-1)**

**2. Compare and contrast the t test in an MLR setting with an SLR setting.**

How are the t tests in the two settings similar and different?

the comparison t, i.e. critical value, in SLR is t0 > t alpha/2, n-2 (which is the same because in SLR k = 1)

Major difference is t in MLR doesn't have us looking in isolation because all other x's/regressors have **not** been held constant.

## 4.6: Confidence Intervals in Multiple Linear Regression

Read Sections 3.4 and 3.5

One at a time CIs for individual regression coefficients and mean response given specific levels of regressors.

**1. Write down the formula for the confidence interval of the coefficient for xj.**

assuming normality of errors, and mean 0 of errors

assume yi is normally distributed with mean of B0 + sum(j = 1:k(Bj * xij)) - remember we are looking at yi, so it is y's at that x value.

**Bj-hat +- t(alpha/2, n-p) * sqrt(sigma^2 * Cjj)**

remember sqrt(sigma^2 * Cjj) = se(Bj-hat)

```{r}
qt(.975,22) #critical value of two tailed 95% CI for n-p == 22 (25 data points, 2 regressors)
```
Notice that the summary table gives the B-hat for each predictor/regressor variable, and gives MS-R so construction of CIs for residuals is really straightforward.


**2. Write down the formula for the confidence interval of the mean response. **

x0 is a transpose of a row from X - so it is a column vector representing the x values for the equation (with 1 for B0 position). So you are calculated the mean response for an observation with 1 for x11.

CI for mean is:

**Y0-hat +- (t(alpha/2,n-p) * sqrt(sigma^2 * (x0')(X'X)^-1(x0))


**Interpretation**: 95% of such intervals will contain the true delivery time.

**3. Write down the formula for the prediction interval of a new response. **

y0-hat = (x0')(B-hat)

A CI is

**Y0-hat +- (t(alpha/2,n-p)(sqrt(sigma^2 * (1 + (x0')(X'X)^-1(x0))))**

**4. Describe, in your own words, why the Bonferroni method is used, and how you would use it.**

significance level alpha is possibility of wrongly rejecting the null hypothesis.

In the context of a CI, alpha is the proportion of random samples that will result in a CI that does not contain the true value of the population parameter.

_____

Bonferroni method is used to select a delta to help obtain a joint/simultaneous confidence interval for Bj-hat

Select a delta multiplier = t(alpha/2p,n-p)

Bj-hat +- delta * se(Bj-hat)

remember diagonal elements in the (X'X)^-1 is B-hat, so se(Bj-hat) == sqrt((sigma^2 * bjj))

Bonferroni allows us a 95% CI for all B's instead of just one. If we have n intervals, we want 95% confidence in all n intervals.

Bonferroni intervals are easier to construct that a confidence ellipse, but you can get a CI for joint intervals of B.

when p gets large, then CIs get really large.

Is it p or g for. p == number of parameters in the model.
g == the number of intervals you are creating.

g = is more flexible, because only represents the number of intervals you need to construct.


# RECAP:
he likes "when controlling for the effect of other predictors"

**Interpretation of Bj**

Page 68:
"the parameter Bj represents the expected change in the response y per unit change in xj when all the remaining regressor variables xi (i != j) are held constant.


**Interpretation of ANOVA / hypothesis tests**
ANOVA F test is not very useful in MLR except in some settings. Will explore more in Module 5.



**t test in MLR**
tests if an individual coefficient is 0. Can we drop a particular xj from the model in the presence of the other predictors.

If xj is highly correlated with at least one other predictor, or a linear combination of other predictors, xj will probably be insignificant as the addition of xj doesn't improve our model. xj MAY still be linearly related to the response variable, but not needed for the model.

"Only not useful **in the presence of the other predictors.**"

**SLR vs. MLR**

SLR: 2 variables and 2 variables oNLY

MLR: response and multiple predictors simultaneously. Can improve model, but if you are only exploring 2 variables you don't need multiple linear regression.





