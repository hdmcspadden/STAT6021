acf(cornModel.t.filtered$residuals, main="ACF of Residuals: Filtered 1/y")
summary(cornModel.t.filtered)
summary(cornModel.t.filtered)
anova(cornModel.t.filtered)
summary(cornModel.yield2.sqrtx)
anova(cornModel.yield2.sqrtx)
(exp(-0.44993) - 1) * 100
exp(1.50792)
yield.t.filtered = 1/yield
cornModel.t.filtered <-lm(yield.t.filtered~nitrogen)
par(mfrow=c(1,2)) # 1 rows, 2 columns - there will be three panes to look through
# scatter plot filtered transformed
plot(x=nitrogen, y=yield.t.filtered, main='Scatter Plot Filtered 1/y', xlab = 'nitrogen', ylab = '1/y')
abline(cornModel.filtered.yield2.sqrtx,col="red")
#residual plot filtered transformed
plot(cornModel.t.filtered$fitted.values,cornModel.t.filtered$residuals, main="Residual Plot: filtered 1/y")
abline(h=0,col="red")
#QQ plot of filtered transformed
qqnorm(cornModel.t.filtered$residuals)
qqline(cornModel.t.filtered$residuals, col="red")
#ACF plot
acf(cornModel.t.filtered$residuals, main="ACF of Residuals: Filtered 1/y")
anova(cornModel.t.filtered)
yield.t.filtered = 1/yield
cornModel.t.filtered <-lm(yield.t.filtered~nitrogen)
par(mfrow=c(1,2)) # 1 rows, 2 columns - there will be three panes to look through
# scatter plot filtered transformed
plot(x=nitrogen, y=yield.t.filtered, main='Scatter Plot Filtered 1/y', xlab = 'nitrogen', ylab = '1/y')
abline(cornModel.filtered.yield2.sqrtx,col="red")
#residual plot filtered transformed
plot(cornModel.t.filtered$fitted.values,cornModel.t.filtered$residuals, main="Residual Plot: filtered 1/y")
abline(h=0,col="red")
#QQ plot of filtered transformed
qqnorm(cornModel.t.filtered$residuals)
qqline(cornModel.t.filtered$residuals, col="red")
#ACF plot
acf(cornModel.t.filtered$residuals, main="ACF of Residuals: Filtered 1/y")
anova(cornModel.t.filtered)
yield.t.filtered = 1/yield
cornModel.t.filtered <-lm(yield.t.filtered~nitrogen)
par(mfrow=c(1,2)) # 1 rows, 2 columns - there will be three panes to look through
# scatter plot filtered transformed
plot(x=nitrogen, y=yield.t.filtered, main='Scatter Plot Filtered 1/y', xlab = 'nitrogen', ylab = '1/y')
abline(cornModel.t.filtered,col="red")
#residual plot filtered transformed
plot(cornModel.t.filtered$fitted.values,cornModel.t.filtered$residuals, main="Residual Plot: filtered 1/y")
abline(h=0,col="red")
#QQ plot of filtered transformed
qqnorm(cornModel.t.filtered$residuals)
qqline(cornModel.t.filtered$residuals, col="red")
#ACF plot
acf(cornModel.t.filtered$residuals, main="ACF of Residuals: Filtered 1/y")
anova(cornModel.t.filtered)
5.914^2
34.9754 * 270
70247.4268 / 34.9754
67247.4268 / 34.9754
5.914^2
SSR = MSR * 270
MSR = 5.914^2
SSR = MSR * 270
MSR = 5.914^2
MSR
SSR = MSR * 270
SSR
SST = SSR / (1 - .8115)
SST = SSR / (1 - .8115)
SST
MSE = 5.914^2
MSE
SSE = MSE * 270
SSE
SST = SSE / (1 - .8115)
SST
SSR = SST - SSE
SSR = SST - SSE
SSR = SST - SSE
SSR
MSR = SSR / 1
MSR = SSR / 1
MSR
FSTAT = MSR / MSE
FSTAT
criticalvalue2d = qt(.0975,270)
criticalvalue2d
criticalvalue2d = qt(.975,270)
criticalvalue2d
criticalvalue2d = qt(.975,270)
criticalvalue2d
yhatforx = 33.4744 + (10.7296 * 4.5)
yhatforx
criticalvalue2d = qt(.975,270)
print(paste("Our Critical Value: ",criticalvalue2d))
yhatforx = 33.4744 + (10.7296 * 4.5)
print(paste("Our y0-hat: ",yhatforx))
print(paste("Our MS-residual: ",MSE))
criticalvalue2d = qt(.975,270)
print(paste("Our Critical Value: ",criticalvalue2d))
yhatforx = 33.4744 + (10.7296 * 4.5)
print(paste("Our y0-hat: ",yhatforx))
print(paste("Our MS-residual: ",MSE))
n = 272
print(paste("Our n: ",n))
criticalvalue2d = qt(.975,270)
print(paste("Our Critical Value: ",criticalvalue2d))
yhatforx = 33.4744 + (10.7296 * 4.5)
print(paste("Our y0-hat: ",yhatforx))
print(paste("Our MS-residual: ",MSE))
n = 272
print(paste("Our n: ",n))
x0 = 4.5
print(paste("Our x0: ", x0))
criticalvalue2d = qt(.975,270)
print(paste("Our Critical Value: ",criticalvalue2d))
yhatforx = 33.4744 + (10.7296 * 4.5)
print(paste("Our y0-hat: ",yhatforx))
print(paste("Our MS-residual: ",MSE))
n = 272
print(paste("Our n: ",n))
OneOvern = 1/272
OneOvern
x0 = 4.5
print(paste("Our x0: ", x0))
criticalvalue2d = qt(.975,270)
print(paste("Our Critical Value: ",criticalvalue2d))
yhatforx = 33.4744 + (10.7296 * 4.5)
print(paste("Our y0-hat: ",yhatforx))
print(paste("Our MS-residual: ",MSE))
n = 272
print(paste("Our n: ",n))
OneOvern = 1/272
print(paste("Our 1/n: ",OneOvern))
x0 = 4.5
print(paste("Our x0: ", x0))
progress1 = 82.7022 - 81.7576
progress2 = progress1 / 1.969
progress1 = 82.7022 - 81.7576
progress2 = progress1 / 1.969
progress2
progress1 = 82.7022 - 81.7576
progress2 = progress1 / 1.969
#progress2
progress3 = progress2^2
#progress3
progress1 = 82.7022 - 81.7576
progress2 = progress1 / 1.969
#progress2
progress3 = progress2^2
progress3
progress1 = 82.7022 - 81.7576
progress2 = progress1 / 1.969
#progress2
progress3 = progress2^2
#progress3
progress4  = progress3 / MSE
progress1 = 82.7022 - 81.7576
progress2 = progress1 / criticalvalue2d
#progress2
progress3 = progress2^2
#progress3
progress4  = progress3 / MSE
progress1 = 82.7022 - 81.7576
progress2 = progress1 / criticalvalue2d
#progress2
progress3 = progress2^2
#progress3
progress4  = progress3 / MSE
progress4
progress1 = 82.7022 - 81.7576
progress2 = progress1 / criticalvalue2d
progress2
progress3 = progress2^2
progress3
progress4  = progress3 / MSE
progress4
progress1 = 82.7022 - 81.7576
progress2 = progress1 / criticalvalue2d
progress2
progress3 = progress2^2
progress3
progress4  = progress3 / MSE
progress4
UNKNOWNPART = progress4 - OneOvern
UNKNOWNPART
progress1 = 82.7022 - 81.7576
progress2 = progress1 / criticalvalue2d
#progress2
progress3 = progress2^2
#progress3
progress4  = progress3 / MSE
#progress4
UNKNOWNPART = progress4 - OneOvern
print(paste("Our UNKNOWNPART is: ",UNKNOWNPART))
lower.prediction  = 81.7576 - ((criticalvalue2d) * sqrt((MSE) * (1 + OneOvern + UNKNOWNPART)))
lower.prediction  = 81.7576 - ((criticalvalue2d) * sqrt((MSE) * (1 + OneOvern + UNKNOWNPART)))
upper.prediction = 81.7576 + ((criticalvalue2d) * sqrt((MSE) * (1 + OneOvern + UNKNOWNPART)))
print(paste("Our 95% CI prediction interval for x of 4.5 is: ", lower.prediction, " - ", upper.prediction))
# Knowns
n <- 19
p <- 2
MSE <- 22.6^2 #use the model with point removed
ei <- 120.829070
hii <- 0.23960510
Si2 <- (((n-p) * MSE) - (ei^2/ (1 - hii))) / (n - p - 1)
print(paste("Sihat^2: ", Si2))
ti <- ei / sqrt((Si2 * (1-hii)))
print(paste("ti: ", ti))
library(ggplot2)
attach(swiss)
#?swiss
swissModel <- lm(Fertility ~ Education + Catholic + Infant.Mortality)
##residuals
#regResiduals <- swissModel$residuals # regular residuals
##studentized residuals
#studentResiduals <- rstandard(swissModel) # rstandard give studentized residuals == ri
##externally studentized residuals
extStudentResiduals <- rstudent(swissModel) # externally studentized
n <- nrow(swiss)
p <- 4
##critical value using Bonferroni procedure
criticalValue <- qt(1-(0.05/(2*n)), n-p-1)
criticalValue
sort(extStudentResiduals)
plot.new()
plot(extStudentResiduals,main="Externally Studentized Residuals", ylim=c(-4,4))
abline(h=criticalValue, col="red")
abline(h=-criticalValue, col="red")
extStudentResiduals[abs(extStudentResiduals)>criticalValue]
lev <- lm.influence(swissModel)$hat
comparisonValue <- 2*p/n
comparisonValue
sort(lev)
plot(lev, main="Leverages", ylim=c(0,0.5))
abline(h=2*p/n, col="red")
DFFITS <- dffits(swissModel)
DFFITS[abs(DFFITS)>2*sqrt(p/n)] # how drastically fitted value changes with and without the presence of the observation
#DFBETAS -
DFBETAS <- dfbetas(swissModel) # measures how estimated coefficients change without presence of the observation
DFBETAS[abs(DFBETAS)>2/sqrt(n)]
# how to find the data point
DFBETAS # can find that value with eyeballing
# COOKS considered how fitted values changes for all values (not just the observation) when data point is removed.
COOKS<-cooks.distance(swissModel)
COOKS[COOKS>qf(0.5,p,n-p)] # f distribution
# Knowns
n <- 19
p <- 2
MSE <- 22.6^2 #use the model with point removed
ei <- 120.829070
hii <- 0.23960510
Si2 <- (((n-p) * MSE) - (ei^2/ (1 - hii))) / (n - p - 1)
print(paste("Sihat^2: ", Si2))
ti <- ei / sqrt((Si2 * (1-hii)))
print(paste("ti: ", ti))
criticalValue <- qt(1-(0.05/(2*n)), n-p-1)
print(paste("critical value for comparison: ", criticalValue))
leverageComparison <- (2 * p) / n
leverageComparison
#hii
#ti
DFFITSi6 <- (sqrt(hii / (1 - hii)) * ti)
print(paste("DFFITSi for observation 6: ", DFFITSi6))
ri = ei / (sqrt(MSE * (1 - hii)))
Di = (ri^2 / p) * (hii / (1 - hii))
criticalValueCooks = qf(0.5,p,n-p)
print(paste("Cook's Distance for Observation 6: ", Di))
print(paste("Cook's Distance F Comparison: ", criticalValueCooks))
# Knowns
n <- 19
p <- 2
MSE <- 40.13^2
ei <- 120.829070
hii <- 0.23960510
Si2 <- (((n-p) * MSE) - (ei^2/ (1 - hii))) / (n - p - 1)
print(paste("Sihat^2: ", Si2))
ti <- ei / sqrt((Si2 * (1-hii)))
print(paste("ti: ", ti))
#hii
#ti
DFFITSi6 <- (sqrt(hii / (1 - hii)) * ti)
print(paste("DFFITSi for observation 6: ", DFFITSi6))
ri = ei / (sqrt(MSE * (1 - hii)))
Di = (ri^2 / p) * (hii / (1 - hii))
criticalValueCooks = qf(0.5,p,n-p)
print(paste("Cook's Distance for Observation 6: ", Di))
print(paste("Cook's Distance F Comparison: ", criticalValueCooks))
ri = ei / (sqrt(MSE * (1 - hii)))
ri
Di = (ri^2 / p) * (hii / (1 - hii))
criticalValueCooks = qf(0.5,p,n-p)
print(paste("Cook's Distance for Observation 6: ", Di))
print(paste("Cook's Distance F Comparison: ", criticalValueCooks))
ei
ri = ei / (sqrt(MSE * (1 - hii)))
#ri
Di = (ri^2 / p) * (hii / (1 - hii))
criticalValueCooks = qf(0.5,p,n-p)
print(paste("Cook's Distance for Observation 6: ", Di))
print(paste("Cook's Distance F Comparison: ", criticalValueCooks))
#ei
MSE
ri = ei / (sqrt(MSE * (1 - hii)))
#ri
Di = (ri^2 / p) * (hii / (1 - hii))
criticalValueCooks = qf(0.5,p,n-p)
print(paste("Cook's Distance for Observation 6: ", Di))
print(paste("Cook's Distance F Comparison: ", criticalValueCooks))
# Knowns
n <- 19
p <- 2
#MSE <- 40.13^2
MSE <- 22.6^2
ei <- 120.829070
hii <- 0.23960510
Si2 <- (((n-p) * MSE) - (ei^2/ (1 - hii))) / (n - p - 1)
print(paste("Sihat^2: ", Si2))
ti <- ei / sqrt((Si2 * (1-hii)))
print(paste("ti: ", ti))
# Knowns
n <- 19
p <- 2
MSE <- 40.13^2
ei <- 120.829070
hii <- 0.23960510
Si2 <- (((n-p) * MSE) - (ei^2/ (1 - hii))) / (n - p - 1)
print(paste("Sihat^2: ", Si2))
ti <- ei / sqrt((Si2 * (1-hii)))
print(paste("ti: ", ti))
compares twice the ln liklihood for full model to twice ln liklihood of reduced model
* chi-square distribution with df = parameters full - parameters reduced
* if Liklihood ratio exceeds chi-square critical value then we reject that we can use the reduced model and we need the full model.
* large value indicates at least one regressor variable in the model is important because non zero regression coefficient.
2. Goodness of fit
* use Liklihood Test Ratio for saturated model and full model.
* calculates deviance
* small deviance (or large P value) indicate model provides good fit.
* chi-square distribution with n-p df
* compare D / (n-p) to unity. if close, then good fit.
3. Pearson chi-square goodness of fit
* chi-square n - p df
* small value (or large P value) indicate model provides good fit.
* compare Pearson / (n-p) to unity. If greatly exceeds unity, then fit is questionable.
4. IF no replicated regressors, then can use **Hosmer-Lemeshow**
* observations classified into g groups based on prob of success.
* compares observed and expected frequencies of observation success.
* chi-square distribution with g - 2 df
* large HL values indicate the model is not n adequate fit.
* compare HL to g - p , and compare that to unity. If
setwd("~/UVaCode/R/STAT6021/Module09")
setwd("~/UVaCode/R/STAT6021/Module09")
library(readr)
wcgs <- read_csv("wcgs.csv")
View(wcgs)
# load the data
library(readr)
wcgs <- read_csv("wcgs.csv")
# load the data
library(readr)
wcgs <- read_csv("wcgs.csv")
attach(wcgs)
boxplot(age~chd69)
boxplot(sbp~chd69)
boxplot(dbp~chd69)
boxplot(ncigs~chd69)
library(ggplot2)
# Create a Boxplot
ggplot(wcgs, aes(x = chd69, y = age)) +
geom_boxplot()
ggplot(wcgs, aes(x = chd69, y = sbp)) +
geom_boxplot()
ggplot(wcgs, aes(x = chd69, y = dbp)) +
geom_boxplot()
ggplot(wcgs, aes(x = chd69, y = ncigs)) +
geom_boxplot()
library(ggplot2)
wcgs$chd69 <- as.factor(wcgs$chd69)
# Create a Boxplot
ggplot(wcgs, aes(x = chd69, y = age)) +
geom_boxplot()
ggplot(wcgs, aes(x = chd69, y = sbp)) +
geom_boxplot()
ggplot(wcgs, aes(x = chd69, y = dbp)) +
geom_boxplot()
ggplot(wcgs, aes(x = chd69, y = ncigs)) +
geom_boxplot()
model <- glm(chd69 ~ age + sbp + dbp + ncigs, family = "binomial")
summary(model)
exp(0.024)
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
y = 1 / (1 + exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)))
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
y = 1 / (1 + exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)))
y
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
y = 1 / (1 + exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)))
y
exp(y)
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
y = 1 / (1 + exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)))
y
log(y)
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
y = 1 / (1 + exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)))
y
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
y = 1 / (1 + exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)))
y
odds = log(.97 / (1- .97))
odds
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
y = exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)) / (1 + exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)))
y
odds = log(.97 / (1- .97))
odds
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
y = exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)) / (1 + exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)))
y
odds = log(y / (1 - y))
odds
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
Eyi = exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)) / (1 + exp(-9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)))
Eyi
odds = log(Eyi / (1 - Eyi))
odds
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
linearPortion <- -9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)
Eyi = exp(linearPortion) / (1 + exp(linearPortion))
Eyi
odds = log(Eyi / (1 - Eyi))
odds
data<-read.table("titanic.txt", header=TRUE, sep="")
n <- 3154
p <- 5
df <- n - p
1-pchisq(model$deviance,df)
n <- 3154
p <- 5
df <- n - p
1-pchisq(model$deviance,df)
n <- 3154
p <- 5
df <- n - p
model$deviance
1-pchisq(model$deviance,df)
n <- 3154
p <- 5
df <- n - p
model$deviance
1-pchisq(model$deviance,df)
1-pchisq(model$null.deviance-model$deviance,4)
x_age <- 45
x_sbp <- 110
x_dbp <- 70
x_ncigs <- 0
linearPortion <- -9.11 + (0.0666 * x_age) + (0.0195 * x_sbp) + (0.0079 * x_dbp) + (0.024 * x_ncigs)
Eyi = exp(linearPortion) / (1 + exp(linearPortion))
Eyi
odds = (Eyi / (1 - Eyi))
odds
testDBPModel <- glm(chd69 ~ age + sbp + ncigs + dbp, family = "binomial")
summary(testDBPModel)
BjHat <- 0.007867
seBjHat <- 0.010057
waldTestValue <- BjHat / seBjHat
waldTestValue
reducedModel <- glm(chd69 ~ age + ncigs, family = "binomial")
1-pchisq(reducedModel$deviance-model$deviance,2)
data<-read.table("titanic.txt", header=TRUE, sep="")
attach(data)
result<-glm(Survived ~ Fare, family = "binomial") #generalized linear model, with binomial family, LR = Gaussian family
summary(result) # Z value is the Wald test
# for CI of coefficients use confint
confint(result, level=0.95)
#consider fell model with 3 predictors
full<-glm(Survived ~ Fare + Sex + Age, family = "binomial")
summary(full)
1-pchisq(full$deviance,703)
1-pchisq(full$null.deviance-full$deviance,3)
1-pchisq(result$deviance-full$deviance,2) # if want to remove last two predictors == partial F test, df = 2 because removing 2 predictors.
##Example 2
# grouped data
data2<-read.table("dose.txt", header=T)
