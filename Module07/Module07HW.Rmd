---
title: "Module 07 Homework"
author: "Diana McSpadden"
date: "10/10/2020"
output:
  word_document: default
  html_document: default
---
# Stat 6021: Homework Set 7


## Name: H. Diana McSpadden
### UID: hdm5s
### 10.15.2020

**Attended Study Group With**:

Caprill Wright, Abby Bernhardt, Jing Huang, Katie Barbre, Michael Kastanowski, Brian, Nam, Srinivasa Chivaluri


#### Attended Study Group With

You will continue to use the birthwt data set from the MASS package for this question.

The data were collected at Baystate Medical Center, Springfield, Mass during 1986.

The data contain information regarding weights of newborn babies as well as a number of potential predictors. Before proceeding, be sure to read the documentation about the data set by typing ?birthwt. The goal of the data set is to relate the birthweight of newborns with the characteristics of their mothers during pregnancy.

```{r q0-1}
# attach the data
library(MASS)

attach(birthwt)
head(birthwt,5)

```

The birthwt data frame has 189 rows and 10 columns. The data were collected at Baystate Medical Center, Springfield, Mass during 1986.

low: indicator of birth weight less than 2.5 kg.

age: mother's age in years.

lwt: mother's weight in pounds at last menstrual period.

race: mother's race (1 = white, 2 = black, 3 = other).

smoke: smoking status during pregnancy.

ptl: number of previous premature labours.

ht: history of hypertension.

ui: presence of uterine irritability.

ftv: number of physician visits during the first trimester.

bwt: birth weight in grams.

## Question 1a) Which of these variables are categorical? 
Ensure that R is viewing the categorical variables correctly (i.e. use the is.numeric() function to check). If needed, use the factor() function to force R to treat the necessary variables as categorical.

```{r q1a-1}

low<-factor(low) #ensure low is a factor

race<-factor(race)#ensure race is a factor
levels(race) <- c("white", "black", "other")
contrasts(race)

smoke<-factor(smoke) #ensure smoke is a factor
ht<-factor(ht) #ensure ht is a factor
ui<-factor(ui) #ensure ui is a factor

head(birthwt)


```

**Answer 1a**

The following predictor variables are categorical:
* low: indicator of birth weight less than 2.5 kg.
* race: mother's race (1 = white, 2 = black, 3 = other).
* smoke: smoking status during pregnancy.
* ht: history of hypertension.
* ui: presence of uterine irritability.

## Question 1b) A classmate of yours makes the following suggestion: 
We should remove the variable low as a predictor for the birth weight of babies." 

Do you agree with your classmate? Briefly explain. Hint: you do not need to do any statistical analysis to answer this question.

**Answer 1b**
Yes, I agree with the classmate. Low is a yes/no (1/0) indicator of whether the baby is low birthweight. It will be perfectly correlated with bwt, and is a categorical response variable: a factor variable of the bwt response variable.

## Question 1c) Based on your answer to part 1b, perform all possible regressions ...
using the regsubsets() function from the leaps package. 

Write down the predictors that lead to a first-order model having the best:

1. adjusted R2,
2. mean-sqaured error,
3. Mallow's Cp,
4. BIC.

**Work on Question 1c**

**First**, use the regsubsets() function
```{r q1c-1}
# add the leaps library
library(leaps)

# create all the regsubsets
# set nbest to 5
allreg <- regsubsets(bwt~age+lwt+race+smoke+ptl+ht+ui+ftv, data=birthwt, nbest=5) # use all variables except low
```

For R2Adj, MSE, Mallows and BIC get the "best" models.

```{r q1c-3}
##create a "data frame" that stores the predictors in the various models considered as well as their various criteria
best <- as.data.frame(summary(allreg)$outmat)

best$adjr2 <- summary(allreg)$adjr2
best$p <- as.numeric(substr(rownames(best),1,1))+1
best$mse <- (summary(allreg)$rss)/(dim(birthwt)[1]-best$p)
best$cp <- summary(allreg)$cp
best$bic <- summary(allreg)$bic

##sort by various criteria
best[order(best$adjr2),] # want largest
best[order(best$mse),] # want smallest
best[order(best$cp),] # want smallest
best[order(best$bic),] # want smallest
```
**Answer 1c**

**Best R2Adj** model uses the following predictors: lwt, race, smoke, ht, ui.

**Best MSE** model uses the following predictors: lwt, race, smoke, ht, ui.

**Best Cp** model uses the following predictors: lwt, race, smoke, ht, ui.

**Best BIC** model uses the following predictors: race, smoke, ht, ui.

## Question d) Based on your answer to part 1b, use backward selection to find the best model according to AIC. 

Start with the first-order model with all the predictors. What is the regression equation selected?

```{r q1d-1}
# only the intercept
regnull <- lm(bwt~1, data=birthwt) # intercept only/empty model == ENDING POINT for backward

#modelAdd <- lm(bwt~age+race)
#summary(modelAdd)

# model with all predictors (except low)
regfull <- lm(bwt~age+race+lwt+smoke+ptl+ht+ui+ftv, data=birthwt) # full model == END POINT

step(regfull, scope=list(lower=regnull, upper=regfull), direction="backward")

```

```{r q1d-2}
modelSelected <- lm(bwt~race+lwt+smoke+ht+ui)
summary(modelSelected)

```

**Answer 1d**

bwt ~ race + lwt + smoke + ht + ui

All numbers are in grams

E(bwt) = 2837.264 - (475.058 IF race is black) - (348.150 IF race is other) + (4.242 * lwt) - (356.321 IF smoke == 1) - (585.193 IF ht == 1) - (525.524 IF UI == 1)

## Question 2) (No R required) The data for this question are 36 monthly observations ...
on variables affecting sales of a product. 

The objective is to determine an efficient model for predicting and explaining market share sales, Share, which is the average monthly market share for the product, in percent. 

The predictors are average monthly price in dollars, price, amount of advertising exposure based on gross Nielson rating, nielsen, whether a discount price was in effect, discount (1 if discount, 0 otherwise), whether a package promotion was in effect, promo (1 if promotion, 0 otherwise), and time in months, time.

**Work on Question 2**

Knowns:
1. y/response variable = Share
2. Regressor/predictor variables:
  1. price - avg monthly price in dollars
  2. nielsen - amount of advertising exposure based on gross Nielson rating
  3. discount: categorical 1 if discount, 0 otherwise
  4. promo: categorical, 1 if under promo, 0 otherwise
  5. time: time in months


## Question 2a) The output below is obtained after using the step() function using forward selection, ...
starting with a model with just the intercept term. What is the model selected based on forward selection?

**Answer 2a**

Share ~ discount + promo + price

## Question 2b) Your client asks you to explain ...
what each step in the output shown above means. Explain the forward selection procedure to your client, for this output.

**Answer 2b**

We start with a prediction based on approximately the average Share value, or what we call the intercept-only model. Essentially, we start without using price, nielsen, discount, promo, or time to help predict Share - so the average Share is the best we could do in that case.

Then we determine the best **single** predictor variable to add to our model by comparing the predictive power of each single predictor model. To compare the predictive power of each of the resulting models we use the statistical criteria: AIC. AIC is a measure of the candidate model's ability to decrease unexplained error. Since we want our model to explain as much of the difference in Share as possible based on the models predictors, we want the smallest AIC value.

The comparison of all your single predictor models resulted in a model with Share based on discount: (Share)~discount, because when comparing AIC's for all the single predictor models, the discount model had the lowest AIC value (-128.137 vs approximately -94 for each of the other candidate models). At this point we made note of the lowest AIC for the selected single predictor models (AIC = -128.137).

We repeated this process for each of the **two** predictor models that use discount as the first predictor, and the model using discount and promo resulted in the lowest AIC. At this point we made note of the lowest AIC for the selected two predictor models (AIC = -129.69) and confirmed that this model has a lower AIC than the single predictor model.

We repeated this process for each of the **three** predictor models that use discount and promo as the first two predictors, and the lowest AIC model used discount, promo and price. At this point we made note of the lowest AIC for the selected three predictor models (AIC = -132.94) and confirmed that this model has a lower AIC than the two predictor model.

When we repeated the examination of all **four** predictor models that use discount, promo, and price as the first three predictors we did not identify a model with a lower AIC than the selected three predictor model. This allowed us to **stop examining** additional predictor variables.

If you are interested the formula for AIC is: (n * ln(SSE / n)) + 2p, where n is the number of observations in the dataset, SSE is the sum squared residual error of the model, and p is the number of parameters the model includes. As predictor variables are added we expect SSE to decrease, but p increases. Our model selection method based on AIC attempts the find the model that balances these two effects.

## Question 2c) Your client asks if he should go ahead and use the models selected in part 2a.
What advice do you have for your client?

**Answer 2c**

I advise that we also investigate which model is identified with backwards selection, and with stepwise selection which are other methods to identify candidate models. After we have identified those models, I recommend investigating the PRESS statistic for each model, and talking with subject matter experts about the ability to collect and maintain the predictor variables in any of the identified models. Again, this is a balancing act to identify the "best" model that will also be maintainable and makes sense to you (the client).

## Question 3) (No R required) Your client asks you to compare and contrast between R2 and the adjusted R2, ...
specifically: name one advantage of R2 over the adjusted R2, and name one advantage of the adjusted R2 over R2.

**Answer 3**

Benefit of **R2**: Comparing R2 is useful when comparing models with identical numbers of variables, and can also be useful when examining a plot of R2p vs. number of parameters an analyst can use judgment to determine the number of regressors for the final model by examining where the curve of the plot becomes apparent.


Benefit of **R2-Adjusted**: R2Adjusted takes into consideration the effect of differing numbers of parameters and the change in degrees of freedom (n - p). R2Adjusted can be used to compare R2Adjusted values of models with different numbers of parameters and if the model with an additional parameter's R2Adjusted value exceeds the R2Adjusted value of the models with fewer predictors, then we know that the additional parameter is significant.


## Question 4) Include the function your group wrote to compute the PRESS statistic 
(Question 2 in Guided Question Set).

```{r q4}
#This is a very literal version of the formula
calculatePRESS <- function(theModel) {
  
  sumPRESS = 0
  hatDiagonals <- lm.influence(theModel)$hat
  
  for (i in 1:length(hatDiagonals)){
    sumPRESS <- sumPRESS + ((theModel$residuals[i]) / (1 - hatDiagonals[i]))^2
  }
  
  return(sumPRESS)
}

# This used the built-in array functions in R
press <- function(theModel) {
  
  pr <- theModel$residuals / (1-lm.influence(theModel)$hat)
  
  return (sum(pr^2))
}
```

## Question 5)
Please remember to complete the Module 7 Guided Question Set Participation Self-and Peer-Evaluation Questions via Test & Quizzes on Collab.

**Answer Question 5**: Will do.


