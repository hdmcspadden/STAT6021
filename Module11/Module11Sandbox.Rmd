---
title: "Module11Sandbox"
author: "Diana McSpadden"
date: "11/8/2020"
output: html_document
---


# Module 11: Regression for Time Series Data and Resampling Methods

## ESSENTIAL QUESTIONS
1. What are the consequences of using linear regression when the observations are not independent?
2. How do we transform the variables to account for the correlation between observations?
3. What are the various methods of cross-validation and what are their pros and cons?

## LEARNING OBJECTIVES
1. Detect the presence of autocorrelation in error terms.
2. Explain how the presence of autocorrelation in error terms affects the least-squares regression model.
3. Describe the setup of an autoregressive (AR) error model, and use the model to answer questions of interest.
4. Explain the pros and cons of the various methods of cross-validation.
5. Use cross-validation to decide which regression model to use in a data analysis.

## Introduction

To account for the correlation between data and error terms, we will use an autoregressive (AR) model for the error terms. After finding the appropriate AR model for the error terms, we can apply a simple transformation to our data, and then fit a linear regression model after the transformation.

In the last part of this lesson, we will compare a few different ways of splitting the data into these two parts, and you will learn about the pros and cons of these different ways to validate models.

## 11.2: Autocorrelation in Errors

Read Sections 14.1 and 14.2 of your textbook. As you read, take notes on the following.
 
### Describe, in your own words, when observations and errors may not be independent in data.
 
* when an important predictor is not included that accounts for changes over time

* actual time dependance


### What are the consequences in ordinary least squares regression when the errors are autocorrelated? 

1. coefficients are not minimum varianceeimates.
2. when errors are + autocorrelated the MSE seriously underestimate the error variance, sigma^2.
    - standard errors of coeffs may be too small
    - CIs will be too small
3. CIs, PIs, hypothesis tests on t or F values are no longer exact.


### How are residual plots used to detect autocorrelation in the errors? 

1. plot residuals vs time. If positive correlation then residuals of equal sign occur in clusters.If negative correlation, changes in sign occur too rapidly.


### Consider the simple linear regression model with first-order autoregressive errors. 

#### (a) Write down the model. 

yt = B0 + B1x1 + Et where Et = (phi * E(t-1)) + at where at is NID(0, sigma^2a)

#### (b) What are the expected value, the variance, and the covariance of the error terms? 

expected value, i.e. mean, of E is E(e) = 0

variance = sigma^2 = sigma^2a  * (1/ (1 - phi^2))

covariance = Equation 14.3 , page 476

#### (c) What is the autocorrelation between two error terms that are k periods apart? 

== phi (Equation 14.3, p 476)


### How is a partial autocorrelation function (PACF) plot used to detect autocorrelation in the errors?

epsilons are a AR(p) structure.


E(t) is a linear combination of past errors and a N(0,sigma^2a) term == a(t)

assess how many past terms to use, i.e. what is "p" in the linearized form of E(t)?

Watson-Durbin assumes it is only 1 (just the last error term == E(t-1)


#### PACF plot of residuals to assess order of AR structure of our errors.

lag h measures correlation of any two points h lags apart after **removing the connections with all the points between x(t) and x(t + h)**

## 11.3: Estimating the Regression Model with Autocorrelated Errors and Using it to Make Predictions

Read Section 14.3 of your textbook. As you read, take notes on the following.
 
Note: There is a mistake in last line of equation (14.7) in textbook. Îµt should be at.
 
### Using your own words, describe the steps you would take to fit a simple linear regression model ...
with first-order autocorrelated errors using the Cochrane-Orcutt method. 

1. get an estimate of phi, using equation 14.8
2. use the phi estimate to transform x' and y' (top of page 483)
3. then use ordinary least squares regression to get B0-hat and B1-hat and new set of residuals.
4. do durbin-watson again to see if d value is good/bad and repeat process if needed with new residuals.

from slides ...

1. Use OLS to get initial coef estimates
2. Examine the AR structure of the sample residuals from step 1 using a PACF plot
3. estimate the coefs phi-1 ... phi-p using ARIMA estimation
4. use estimated phi-1...phi-p to transform y' and x'
5. use OLS to estimate yi and xi
6. If everything works, the residuals at the end of step 5 should be uncorrelated and mean=0 and constant variance.


### What is the forecast at time period T+1 based on the end of the current time period T? 

Eq. 14.15 on page 491

### What is the one-step ahead forecast error? 

a(T + 1)

## 11.4: Cross-Validation
Read from the beginning of Section 5 up to Section 5.1.4 of An Introduction to Statistical Learning, with Applications in R (scroll down to the bottom to find the link to the book). As you read, take notes on the following.
 
### Using your own words, describe the steps taken in the validation set approach. 

1. randomly divide available set into training and test.
2. Fit on Training
2. Predict values in test
5. Determine predict error rate .. MSE for quant responses.


### Using your own words, describe the steps taken in Leave-One-Out Cross-Validation (LOOCV). 

1. split into two parts, but only single observation is used for validation.
2. n of these training vs. 1 models are done so you have MSE1 ... MSEn and find mean of MSE's


### What are the advantages of LOOCV over the validation set approach? 
1. far less bias, tends not to overestimate test error rate
2. no randomness of results. same mean MSE each time with data set.
3. can use equation 5.2: https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf
and only run once.


### Using your own words, describe the steps taken in k-Fold Cross-Validation. 
1. randomly divide into k groups, or folds of approximately equal size
2. 1st fold == validation
3. fit the model with the remaining folds.
4. compute MSE of the validation fold
5. repeat the procedure k times, each time with a different group as the validation fold.
6. calculate the mean.

### How is LOOCV a special case of k-Fold Cross-Validation?


LOOCV is a special case where the validation group size is 1, or k==n.

### Comment on the bias-variance tradeoff between LOOCV and k-Fold Cross-Validation.

k-fold can give more accurate estimates of the test error rate than LOOCV because of bias-variance trade-off.

LOOCV has lower bias because n-1 observations used to fit, BUT k-fold has lower variance in estimates because there is less correlation because the overlaps between training data are smaller.

k==5, or k==10 are preferred.

## 11.5 RECAP

lag regression: regress response on lagged version of predictors.

**cross correlation functions**

ARMA errors: include another q terms in AR error equation with a correction, theta.

bias: whether long run average is == truth - over time it is true

variance: whether "distance" of estimate from truth on average - each value may be far


## Lecture

**Why we transform both variables in time series?**: 

**Time Series REgression**:

still is y = intercept + slopex1 + Ei

but error are autocorrelated ...


it could  ga back several lags.

prewhitening == transform both variables to make one of them white noise.




